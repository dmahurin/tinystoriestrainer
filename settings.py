
tokenizer = "BEE-spoke-data/smol_llama-101M-GQA"
configuration = 'smol-llama-101m'
#dataset_name = 'prepared_wikipedia_en'
dataset_name = 'prepared_tinystories2'
TRAIN_PATH = 'TinyStoriesV2-GPT4-train.txt'
VALID_PATH = 'TinyStoriesV2-GPT4-valid.txt'
END_OF_TEXT = '<|endoftext|>'
MAX_LENGTH = 2048

OUTPUT_DIR = 'results'
